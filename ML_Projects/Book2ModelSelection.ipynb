{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Applying Machine Learning to create “Absenteeism Module”<center>\n",
    "\n",
    "+ Further work carried out for this project was about selction of machine learning algorithm to apply so that we get good accuracyfor model we implement\n",
    "\n",
    "+ We first implemented a 10-fold cross validation method to evaluate each algorithm.\n",
    "\n",
    "+ We also explored the same possibilities by using sklearn to evaluate the algorithms which we took in consideration \n",
    "\n",
    "### Selction of model for implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the cross fold validation method to see which model fits the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to select which machine learining algorithm is best suitable for this dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('Absenteeism_at_work.csv', 'Absenteeism_at_work.csv', delimiter=';')\n",
    "# We directly fed the data without preprocessing it\n",
    "#print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values\n",
    "X = array[:,0:20]\n",
    "Y = array[:,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare configuration for cross validation test harness\n",
    "seed = 10\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('DTC', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('RFR', RandomForestClassifier()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The 10-fold cross validation  is used to evaluate each algorithm, which is configured with the random seed to ensure that the same splits to the training data are performed and that each algorithms is evaluated in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.432432 (0.056044)\n",
      "KNN: 0.354054 (0.042648)\n",
      "DTC: 0.358108 (0.043684)\n",
      "NB: 0.182432 (0.057730)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.304054 (0.037862)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR: 0.406757 (0.048892)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example provides a list of each algorithm short name, the mean accuracy and the standard deviation accuracy.\n",
    "We pasted the results below to make it convinient for reference:-\n",
    "\n",
    "+ Logistic Regression: 0.432432 (0.056044)(43%)\n",
    "+ KNN: 0.354054 (0.042648) (35%)\n",
    "+ Decision Tree Classifier: 0.360811 (0.046832) (35%)\n",
    "+ Naive Bayes: 0.182432 (0.057730) (18%)\n",
    "+ Support Vector Machines: 0.304054 (0.037862)(30%)\n",
    "+ Random Forest Regressor: 0.409459 (0.055734) (40%)\n",
    "\n",
    "From above observation we can say that Logistic Regression and Random Forest Regressor would work as good fit for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ From above box and whisker plot which show the spread of the accuracy scores across each cross validation fold for each algorithm we can say that Logistic regression model is worth to give a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Reason_1</th>\n",
       "      <th>Reason_2</th>\n",
       "      <th>Reason_3</th>\n",
       "      <th>Reason_4</th>\n",
       "      <th>Month of absence</th>\n",
       "      <th>Day of the week</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Transportation expense</th>\n",
       "      <th>Distance from Residence to Work</th>\n",
       "      <th>...</th>\n",
       "      <th>Disciplinary failure</th>\n",
       "      <th>Education</th>\n",
       "      <th>Son</th>\n",
       "      <th>Social drinker</th>\n",
       "      <th>Social smoker</th>\n",
       "      <th>Pet</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>Body mass index</th>\n",
       "      <th>Absenteeism time in hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>171.999997</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  Reason_1  Reason_2  Reason_3  Reason_4  Month of absence  \\\n",
       "0  11.0         0         0         0         1               7.0   \n",
       "1  36.0         0         0         0         0               7.0   \n",
       "2   3.0         0         0         0         1               7.0   \n",
       "3   7.0         1         0         0         0               7.0   \n",
       "4  11.0         0         0         0         1               7.0   \n",
       "\n",
       "   Day of the week  Seasons  Transportation expense  \\\n",
       "0              3.0      1.0                   289.0   \n",
       "1              3.0      1.0                   118.0   \n",
       "2              4.0      1.0                   179.0   \n",
       "3              5.0      1.0                   279.0   \n",
       "4              5.0      1.0                   289.0   \n",
       "\n",
       "   Distance from Residence to Work  ...  Disciplinary failure  Education  Son  \\\n",
       "0                             36.0  ...                   0.0          0  2.0   \n",
       "1                             13.0  ...                   1.0          0  1.0   \n",
       "2                             51.0  ...                   0.0          0  0.0   \n",
       "3                              5.0  ...                   0.0          0  2.0   \n",
       "4                             36.0  ...                   0.0          0  2.0   \n",
       "\n",
       "   Social drinker  Social smoker  Pet  Weight      Height  Body mass index  \\\n",
       "0             1.0            0.0  1.0    90.0  172.000000             30.0   \n",
       "1             1.0            0.0  0.0    98.0  171.999997             31.0   \n",
       "2             1.0            0.0  0.0    89.0  170.000000             31.0   \n",
       "3             1.0            1.0  0.0    68.0  168.000000             24.0   \n",
       "4             1.0            0.0  1.0    90.0  172.000000             30.0   \n",
       "\n",
       "   Absenteeism time in hours  \n",
       "0                        4.0  \n",
       "1                        0.0  \n",
       "2                        2.0  \n",
       "3                        4.0  \n",
       "4                        2.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Absenteeism time in hours'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. By looking at 'Absenteeism in hours' column we see that the median of that column is three. \n",
    "\n",
    "2. By calculating the median for this column above we are going to split it. \n",
    "\n",
    "The classes will be as follows:-\n",
    "\n",
    "1. Moderately Absent(<=3hr) and Excessively absent(>=4hr)\n",
    "\n",
    "2. Using median method is numerically stable and rigid because in this way we balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#np.where(condition,value if true, value if false) \n",
    "#it checks if a condition has been satisfied and assigns a value accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=np.where(df['Absenteeism time in hours']>df['Absenteeism time in hours'].median(),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output is in np array\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Excessive Absenteeism ']=targets\n",
    "# We save these targets to new column \"Excessive Absenteeism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4540540540540541"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.sum()/targets.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The above method will prevent model from learning to output only 0 or only 1.\n",
    "\n",
    "+ Also the above result implies that 46% are 1 and 54% remaining are zero\n",
    "\n",
    "+ From model selection code above we roughly kept in mind that Logistic Regression model is fairly suitable for this\n",
    "\n",
    "+ For regression problem and creating 2 classes ,balance dataset should have 50% of samples exact usually a 60 40 split is preferresd in Logistic regression\n",
    "\n",
    "+ But still despite of our above model selection method we decided to analyse accuracy of other models after splitting the data in train and test variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets=df.drop(['Absenteeism time in hours','Height','Seasons'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We decided to eliminate above columns 'Absenteeism time in hours','Height','Seasons' by referring to backward elemination method \n",
    "+ At first we just eliminated \"Absenteeism in hours\". But after calcultaing odds ratio,we eliminated the other columns which were of less importance to our dataset.\n",
    "+ As we went on dropping these columns one by one and analysed the accuracy, we found that the accuracy incresed as we dropped the columns of least importance.\n",
    "+ As wew were working in parts for this project one of us was trying to implement Logistic Regression model while on other hand  one of us evaluated the accuracy using sklearn technique.\n",
    "+ So while implementing regression model we found out least important columns via backward elimination method. So instead of implementing that method for all consiederd models. We just took decision of elemination by refering to backward elimnation implemented for regression model and hoped that it would work out fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select inputs for building model \n",
    "\n",
    "1.we used iloc method\n",
    "\n",
    "2.df.iloc[row indices,column indices] \n",
    "\n",
    "select(slices)data by position when want particular rows and columns\n",
    "\n",
    "iloc excludes the given row or column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Reason_1</th>\n",
       "      <th>Reason_2</th>\n",
       "      <th>Reason_3</th>\n",
       "      <th>Reason_4</th>\n",
       "      <th>Month of absence</th>\n",
       "      <th>Day of the week</th>\n",
       "      <th>Transportation expense</th>\n",
       "      <th>Distance from Residence to Work</th>\n",
       "      <th>Service time</th>\n",
       "      <th>...</th>\n",
       "      <th>Work load Average/day</th>\n",
       "      <th>Hit target</th>\n",
       "      <th>Disciplinary failure</th>\n",
       "      <th>Education</th>\n",
       "      <th>Son</th>\n",
       "      <th>Social drinker</th>\n",
       "      <th>Social smoker</th>\n",
       "      <th>Pet</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Body mass index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239.554</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>205.917</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275.089</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>264.604</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>271.219</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>271.219</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>271.219</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>740 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Reason_1  Reason_2  Reason_3  Reason_4  Month of absence  \\\n",
       "0    11.0         0         0         0         1               7.0   \n",
       "1    36.0         0         0         0         0               7.0   \n",
       "2     3.0         0         0         0         1               7.0   \n",
       "3     7.0         1         0         0         0               7.0   \n",
       "4    11.0         0         0         0         1               7.0   \n",
       "5     3.0         0         0         0         1               7.0   \n",
       "6    10.0         0         0         0         1               7.0   \n",
       "7    20.0         0         0         0         1               7.0   \n",
       "8    14.0         0         0         1         0               7.0   \n",
       "9     1.0         0         0         0         1               7.0   \n",
       "10   20.0         1         0         0         0               7.0   \n",
       "11   20.0         1         0         0         0               7.0   \n",
       "12   20.0         1         0         0         0               7.0   \n",
       "13    3.0         1         0         0         0               7.0   \n",
       "14    3.0         0         0         0         1               7.0   \n",
       "15   24.0         1         0         0         0               7.0   \n",
       "16    3.0         0         0         0         1               7.0   \n",
       "17    3.0         0         0         1         0               7.0   \n",
       "18    6.0         1         0         0         0               7.0   \n",
       "19   33.0         0         0         0         1               8.0   \n",
       "20   18.0         1         0         0         0               8.0   \n",
       "21    3.0         1         0         0         0               8.0   \n",
       "22   10.0         1         0         0         0               8.0   \n",
       "23   20.0         0         0         0         1               8.0   \n",
       "24   11.0         0         0         1         0               8.0   \n",
       "25   10.0         0         0         0         1               8.0   \n",
       "26   11.0         0         0         0         1               8.0   \n",
       "27   30.0         0         0         0         1               8.0   \n",
       "28   11.0         0         0         1         0               8.0   \n",
       "29    3.0         0         0         0         1               8.0   \n",
       "..    ...       ...       ...       ...       ...               ...   \n",
       "710  23.0         0         0         0         1               6.0   \n",
       "711  36.0         0         0         1         0               6.0   \n",
       "712  12.0         0         0         1         0               6.0   \n",
       "713  22.0         0         0         0         1               6.0   \n",
       "714   2.0         0         0         0         0               6.0   \n",
       "715  21.0         0         0         0         0               6.0   \n",
       "716  36.0         0         0         1         0               6.0   \n",
       "717  22.0         1         0         0         0               6.0   \n",
       "718  15.0         0         0         0         1               6.0   \n",
       "719  22.0         1         0         0         0               6.0   \n",
       "720  34.0         0         0         0         1               6.0   \n",
       "721  12.0         0         0         0         1               6.0   \n",
       "722  34.0         1         0         0         0               6.0   \n",
       "723  34.0         1         0         0         0               6.0   \n",
       "724  12.0         0         0         0         1               6.0   \n",
       "725   5.0         0         0         0         1               7.0   \n",
       "726  12.0         0         0         1         0               7.0   \n",
       "727   9.0         1         0         0         0               7.0   \n",
       "728  34.0         0         0         0         1               7.0   \n",
       "729   9.0         1         0         0         0               7.0   \n",
       "730   6.0         0         0         0         1               7.0   \n",
       "731  34.0         0         0         0         1               7.0   \n",
       "732  10.0         0         0         0         1               7.0   \n",
       "733  28.0         0         0         0         1               7.0   \n",
       "734  13.0         1         0         0         0               7.0   \n",
       "735  11.0         1         0         0         0               7.0   \n",
       "736   1.0         1         0         0         0               7.0   \n",
       "737   4.0         0         0         0         0               0.0   \n",
       "738   8.0         0         0         0         0               0.0   \n",
       "739  35.0         0         0         0         0               0.0   \n",
       "\n",
       "     Day of the week  Transportation expense  Distance from Residence to Work  \\\n",
       "0                3.0                   289.0                             36.0   \n",
       "1                3.0                   118.0                             13.0   \n",
       "2                4.0                   179.0                             51.0   \n",
       "3                5.0                   279.0                              5.0   \n",
       "4                5.0                   289.0                             36.0   \n",
       "5                6.0                   179.0                             51.0   \n",
       "6                6.0                   361.0                             52.0   \n",
       "7                6.0                   260.0                             50.0   \n",
       "8                2.0                   155.0                             12.0   \n",
       "9                2.0                   235.0                             11.0   \n",
       "10               2.0                   260.0                             50.0   \n",
       "11               3.0                   260.0                             50.0   \n",
       "12               4.0                   260.0                             50.0   \n",
       "13               4.0                   179.0                             51.0   \n",
       "14               4.0                   179.0                             51.0   \n",
       "15               6.0                   246.0                             25.0   \n",
       "16               6.0                   179.0                             51.0   \n",
       "17               2.0                   179.0                             51.0   \n",
       "18               5.0                   189.0                             29.0   \n",
       "19               4.0                   248.0                             25.0   \n",
       "20               4.0                   330.0                             16.0   \n",
       "21               2.0                   179.0                             51.0   \n",
       "22               2.0                   361.0                             52.0   \n",
       "23               6.0                   260.0                             50.0   \n",
       "24               2.0                   289.0                             36.0   \n",
       "25               2.0                   361.0                             52.0   \n",
       "26               3.0                   289.0                             36.0   \n",
       "27               4.0                   157.0                             27.0   \n",
       "28               4.0                   289.0                             36.0   \n",
       "29               6.0                   179.0                             51.0   \n",
       "..               ...                     ...                              ...   \n",
       "710              5.0                   378.0                             49.0   \n",
       "711              5.0                   118.0                             13.0   \n",
       "712              6.0                   233.0                             51.0   \n",
       "713              6.0                   179.0                             26.0   \n",
       "714              2.0                   235.0                             29.0   \n",
       "715              2.0                   268.0                             11.0   \n",
       "716              5.0                   118.0                             13.0   \n",
       "717              5.0                   179.0                             26.0   \n",
       "718              5.0                   291.0                             31.0   \n",
       "719              2.0                   179.0                             26.0   \n",
       "720              2.0                   118.0                             10.0   \n",
       "721              5.0                   233.0                             51.0   \n",
       "722              6.0                   118.0                             10.0   \n",
       "723              4.0                   118.0                             10.0   \n",
       "724              4.0                   233.0                             51.0   \n",
       "725              4.0                   235.0                             20.0   \n",
       "726              6.0                   233.0                             51.0   \n",
       "727              2.0                   228.0                             14.0   \n",
       "728              2.0                   118.0                             10.0   \n",
       "729              3.0                   228.0                             14.0   \n",
       "730              3.0                   189.0                             29.0   \n",
       "731              4.0                   118.0                             10.0   \n",
       "732              4.0                   361.0                             52.0   \n",
       "733              4.0                   225.0                             26.0   \n",
       "734              2.0                   369.0                             17.0   \n",
       "735              3.0                   289.0                             36.0   \n",
       "736              3.0                   235.0                             11.0   \n",
       "737              3.0                   118.0                             14.0   \n",
       "738              4.0                   231.0                             35.0   \n",
       "739              6.0                   179.0                             45.0   \n",
       "\n",
       "     Service time  ...  Work load Average/day   Hit target  \\\n",
       "0            13.0  ...                 239.554        97.0   \n",
       "1            18.0  ...                 239.554        97.0   \n",
       "2            18.0  ...                 239.554        97.0   \n",
       "3            14.0  ...                 239.554        97.0   \n",
       "4            13.0  ...                 239.554        97.0   \n",
       "5            18.0  ...                 239.554        97.0   \n",
       "6             3.0  ...                 239.554        97.0   \n",
       "7            11.0  ...                 239.554        97.0   \n",
       "8            14.0  ...                 239.554        97.0   \n",
       "9            14.0  ...                 239.554        97.0   \n",
       "10           11.0  ...                 239.554        97.0   \n",
       "11           11.0  ...                 239.554        97.0   \n",
       "12           11.0  ...                 239.554        97.0   \n",
       "13           18.0  ...                 239.554        97.0   \n",
       "14           18.0  ...                 239.554        97.0   \n",
       "15           16.0  ...                 239.554        97.0   \n",
       "16           18.0  ...                 239.554        97.0   \n",
       "17           18.0  ...                 239.554        97.0   \n",
       "18           13.0  ...                 239.554        97.0   \n",
       "19           14.0  ...                 205.917        92.0   \n",
       "20            4.0  ...                 205.917        92.0   \n",
       "21           18.0  ...                 205.917        92.0   \n",
       "22            3.0  ...                 205.917        92.0   \n",
       "23           11.0  ...                 205.917        92.0   \n",
       "24           13.0  ...                 205.917        92.0   \n",
       "25            3.0  ...                 205.917        92.0   \n",
       "26           13.0  ...                 205.917        92.0   \n",
       "27            6.0  ...                 205.917        92.0   \n",
       "28           13.0  ...                 205.917        92.0   \n",
       "29           18.0  ...                 205.917        92.0   \n",
       "..            ...  ...                     ...         ...   \n",
       "710          11.0  ...                 275.089        96.0   \n",
       "711          18.0  ...                 275.089        96.0   \n",
       "712           1.0  ...                 275.089        96.0   \n",
       "713           9.0  ...                 275.089        96.0   \n",
       "714          12.0  ...                 275.089        96.0   \n",
       "715           8.0  ...                 275.089        96.0   \n",
       "716          18.0  ...                 275.089        96.0   \n",
       "717           9.0  ...                 275.089        96.0   \n",
       "718          12.0  ...                 275.089        96.0   \n",
       "719           9.0  ...                 275.089        96.0   \n",
       "720          10.0  ...                 275.089        96.0   \n",
       "721           1.0  ...                 275.089        96.0   \n",
       "722          10.0  ...                 275.089        96.0   \n",
       "723          10.0  ...                 275.089        96.0   \n",
       "724           1.0  ...                 275.089        96.0   \n",
       "725          13.0  ...                 264.604        93.0   \n",
       "726           1.0  ...                 264.604        93.0   \n",
       "727          16.0  ...                 264.604        93.0   \n",
       "728          10.0  ...                 264.604        93.0   \n",
       "729          16.0  ...                 264.604        93.0   \n",
       "730          13.0  ...                 264.604        93.0   \n",
       "731          10.0  ...                 264.604        93.0   \n",
       "732           3.0  ...                 264.604        93.0   \n",
       "733           9.0  ...                 264.604        93.0   \n",
       "734          12.0  ...                 264.604        93.0   \n",
       "735          13.0  ...                 264.604        93.0   \n",
       "736          14.0  ...                 264.604        93.0   \n",
       "737          13.0  ...                 271.219        95.0   \n",
       "738          14.0  ...                 271.219        95.0   \n",
       "739          14.0  ...                 271.219        95.0   \n",
       "\n",
       "     Disciplinary failure  Education  Son  Social drinker  Social smoker  Pet  \\\n",
       "0                     0.0          0  2.0             1.0            0.0  1.0   \n",
       "1                     1.0          0  1.0             1.0            0.0  0.0   \n",
       "2                     0.0          0  0.0             1.0            0.0  0.0   \n",
       "3                     0.0          0  2.0             1.0            1.0  0.0   \n",
       "4                     0.0          0  2.0             1.0            0.0  1.0   \n",
       "5                     0.0          0  0.0             1.0            0.0  0.0   \n",
       "6                     0.0          0  1.0             1.0            0.0  4.0   \n",
       "7                     0.0          0  4.0             1.0            0.0  0.0   \n",
       "8                     0.0          0  2.0             1.0            0.0  0.0   \n",
       "9                     0.0          1  1.0             0.0            0.0  1.0   \n",
       "10                    0.0          0  4.0             1.0            0.0  0.0   \n",
       "11                    0.0          0  4.0             1.0            0.0  0.0   \n",
       "12                    0.0          0  4.0             1.0            0.0  0.0   \n",
       "13                    0.0          0  0.0             1.0            0.0  0.0   \n",
       "14                    0.0          0  0.0             1.0            0.0  0.0   \n",
       "15                    0.0          0  0.0             1.0            0.0  0.0   \n",
       "16                    0.0          0  0.0             1.0            0.0  0.0   \n",
       "17                    0.0          0  0.0             1.0            0.0  0.0   \n",
       "18                    0.0          0  2.0             0.0            0.0  2.0   \n",
       "19                    0.0          0  2.0             0.0            0.0  1.0   \n",
       "20                    0.0          1  0.0             0.0            0.0  0.0   \n",
       "21                    0.0          0  0.0             1.0            0.0  0.0   \n",
       "22                    0.0          0  1.0             1.0            0.0  4.0   \n",
       "23                    0.0          0  4.0             1.0            0.0  0.0   \n",
       "24                    0.0          0  2.0             1.0            0.0  1.0   \n",
       "25                    0.0          0  1.0             1.0            0.0  4.0   \n",
       "26                    0.0          0  2.0             1.0            0.0  1.0   \n",
       "27                    0.0          0  0.0             1.0            1.0  0.0   \n",
       "28                    0.0          0  2.0             1.0            0.0  1.0   \n",
       "29                    0.0          0  0.0             1.0            0.0  0.0   \n",
       "..                    ...        ...  ...             ...            ...  ...   \n",
       "710                   0.0          0  2.0             0.0            1.0  4.0   \n",
       "711                   0.0          0  1.0             1.0            0.0  0.0   \n",
       "712                   0.0          1  1.0             1.0            0.0  8.0   \n",
       "713                   0.0          1  0.0             0.0            0.0  0.0   \n",
       "714                   1.0          0  1.0             0.0            1.0  5.0   \n",
       "715                   1.0          1  0.0             0.0            0.0  0.0   \n",
       "716                   0.0          0  1.0             1.0            0.0  0.0   \n",
       "717                   0.0          1  0.0             0.0            0.0  0.0   \n",
       "718                   0.0          0  1.0             1.0            0.0  1.0   \n",
       "719                   0.0          1  0.0             0.0            0.0  0.0   \n",
       "720                   0.0          0  0.0             0.0            0.0  0.0   \n",
       "721                   0.0          1  1.0             1.0            0.0  8.0   \n",
       "722                   0.0          0  0.0             0.0            0.0  0.0   \n",
       "723                   0.0          0  0.0             0.0            0.0  0.0   \n",
       "724                   0.0          1  1.0             1.0            0.0  8.0   \n",
       "725                   0.0          0  1.0             1.0            0.0  0.0   \n",
       "726                   0.0          1  1.0             1.0            0.0  8.0   \n",
       "727                   0.0          0  2.0             0.0            0.0  1.0   \n",
       "728                   0.0          0  0.0             0.0            0.0  0.0   \n",
       "729                   0.0          0  2.0             0.0            0.0  1.0   \n",
       "730                   0.0          0  2.0             0.0            0.0  2.0   \n",
       "731                   0.0          0  0.0             0.0            0.0  0.0   \n",
       "732                   0.0          0  1.0             1.0            0.0  4.0   \n",
       "733                   0.0          0  1.0             0.0            0.0  2.0   \n",
       "734                   0.0          0  3.0             1.0            0.0  0.0   \n",
       "735                   0.0          0  2.0             1.0            0.0  1.0   \n",
       "736                   0.0          1  1.0             0.0            0.0  1.0   \n",
       "737                   0.0          0  1.0             1.0            0.0  8.0   \n",
       "738                   0.0          0  2.0             1.0            0.0  2.0   \n",
       "739                   0.0          0  1.0             0.0            0.0  1.0   \n",
       "\n",
       "     Weight  Body mass index  \n",
       "0      90.0             30.0  \n",
       "1      98.0             31.0  \n",
       "2      89.0             31.0  \n",
       "3      68.0             24.0  \n",
       "4      90.0             30.0  \n",
       "5      89.0             31.0  \n",
       "6      80.0             27.0  \n",
       "7      65.0             23.0  \n",
       "8      95.0             25.0  \n",
       "9      88.0             29.0  \n",
       "10     65.0             23.0  \n",
       "11     65.0             23.0  \n",
       "12     65.0             23.0  \n",
       "13     89.0             31.0  \n",
       "14     89.0             31.0  \n",
       "15     67.0             23.0  \n",
       "16     89.0             31.0  \n",
       "17     89.0             31.0  \n",
       "18     69.0             25.0  \n",
       "19     86.0             32.0  \n",
       "20     84.0             25.0  \n",
       "21     89.0             31.0  \n",
       "22     80.0             27.0  \n",
       "23     65.0             23.0  \n",
       "24     90.0             30.0  \n",
       "25     80.0             27.0  \n",
       "26     90.0             30.0  \n",
       "27     75.0             22.0  \n",
       "28     90.0             30.0  \n",
       "29     89.0             31.0  \n",
       "..      ...              ...  \n",
       "710    65.0             21.0  \n",
       "711    98.0             31.0  \n",
       "712    68.0             21.0  \n",
       "713    56.0             19.0  \n",
       "714    88.0             33.0  \n",
       "715    79.0             25.0  \n",
       "716    98.0             31.0  \n",
       "717    56.0             19.0  \n",
       "718    73.0             25.0  \n",
       "719    56.0             19.0  \n",
       "720    83.0             28.0  \n",
       "721    68.0             21.0  \n",
       "722    83.0             28.0  \n",
       "723    83.0             28.0  \n",
       "724    68.0             21.0  \n",
       "725   106.0             38.0  \n",
       "726    68.0             21.0  \n",
       "727    65.0             22.0  \n",
       "728    83.0             28.0  \n",
       "729    65.0             22.0  \n",
       "730    69.0             25.0  \n",
       "731    83.0             28.0  \n",
       "732    80.0             27.0  \n",
       "733    69.0             24.0  \n",
       "734    70.0             25.0  \n",
       "735    90.0             30.0  \n",
       "736    88.0             29.0  \n",
       "737    98.0             34.0  \n",
       "738   100.0             35.0  \n",
       "739    77.0             25.0  \n",
       "\n",
       "[740 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targets.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs=df_targets.iloc[:,:-1]\n",
    "# Selected inputs stored in unscaled_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing and scaling the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Import the libraries needed to create the Custom Scaler,note that all of them are a part of the sklearn package\n",
    "+ One of them is actually the StandardScaler module, so you can imagine that the Custom Scaler is build on it.\n",
    "+ We use custome scaler to prevent dummies from being modified for which we created columns to scale\n",
    "+ By using custome scaler we prevent the dummies from being standardize which causes to loose interpretability of dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the Custom Scaler class\n",
    "\n",
    "class CustomScaler(BaseEstimator,TransformerMixin): \n",
    "    \n",
    "    # init or what information we need to declare a CustomScaler object\n",
    "    #it will only standardize those inputs which we want\n",
    "    # and what is calculated/declared as we do\n",
    "    \n",
    "    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n",
    "        \n",
    "        # scaler is nothing but a Standard Scaler object\n",
    "        self.scaler = StandardScaler(copy,with_mean,with_std)\n",
    "        # with some columns 'twist'\n",
    "        self.columns = columns\n",
    "        self.mean_ = None\n",
    "        self.var_ = None\n",
    "        \n",
    "    \n",
    "    # the fit method, which, again based on StandardScale\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns], y)\n",
    "        self.mean_ = np.mean(X[self.columns])\n",
    "        self.var_ = np.var(X[self.columns])\n",
    "        return self\n",
    "    \n",
    "    # the transform method which does the actual scaling\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        \n",
    "        # record the initial order of the columns\n",
    "        init_col_order = X.columns\n",
    "        \n",
    "        # scale all features that you chose when creating the instance of the class\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n",
    "        \n",
    "        # declare a variable containing all information that was not scaled\n",
    "        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n",
    "        \n",
    "        # return a data frame which contains all scaled features and all 'not scaled' features\n",
    "       \n",
    "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ID', 'Reason_1', 'Reason_2', 'Reason_3', 'Reason_4',\n",
       "       'Month of absence', 'Day of the week', 'Transportation expense',\n",
       "       'Distance from Residence to Work', 'Service time', 'Age',\n",
       "       'Work load Average/day ', 'Hit target', 'Disciplinary failure',\n",
       "       'Education', 'Son', 'Social drinker', 'Social smoker', 'Pet',\n",
       "       'Weight', 'Body mass index'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check what are all columns that we've got\n",
    "unscaled_inputs.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the columns to scale\n",
    "# we later augmented this code and put it in comments\n",
    "# columns_to_scale = ['Month Value','Day of the Week', 'Transportation Expense', 'Distance to Work',\n",
    "       #'Age', 'Daily Work Load Average', 'Body Mass Index', 'Children', 'Pet',Height','Seasons'']\n",
    "    \n",
    "# select the columns to omit\n",
    "#We omitted these columns to remove dummies\n",
    "columns_to_omit = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4','Education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the columns to scale, based on the columns to omit\n",
    "# use list comprehension to iterate over the list\n",
    "columns_to_scale = [x for x in unscaled_inputs.columns.values if x not in columns_to_omit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a scaler object, specifying the columns you want to scale\n",
    "absenteeism_scaler = CustomScaler(columns_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomScaler(columns=['ID', 'Month of absence', 'Day of the week', 'Transportation expense', 'Distance from Residence to Work', 'Service time', 'Age', 'Work load Average/day ', 'Hit target', 'Disciplinary failure', 'Son', 'Social drinker', 'Social smoker', 'Pet', 'Weight', 'Body mass index'],\n",
       "       copy=None, with_mean=None, with_std=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line will calculate and store the mean and the standard deviation.\n",
    "#Standardization info is present in absenteeism_scaler\n",
    "# fit the data (calculate mean and standard deviation); they are automatically stored inside the object \n",
    "absenteeism_scaler.fit(unscaled_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizes the data, using the transform method \n",
    "# in the last line, we fitted the data - in other words\n",
    "# we found the internal parameters of a model that will be used to transform data. \n",
    "# transforming applies these parameters to our data\n",
    "# Also when you get new data, you can just call 'scaler' again and transform it in the same way as now\n",
    "scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)\n",
    "#in this we actually do subtration of mean and divide by standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(740, 21)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[           ID  Reason_1  Reason_2  Reason_3  Reason_4  Month of absence  \\\n",
       " 87  -0.273981         1         0         0         0          1.361597   \n",
       " 719  0.361586         1         0         0         0         -0.094446   \n",
       " 26  -0.637161         0         0         0         1          0.487971   \n",
       " 188 -0.364776         0         0         0         1         -0.676863   \n",
       " 31  -1.454318         0         0         1         0          0.487971   \n",
       " 93   0.179995         0         0         0         1          1.361597   \n",
       " 102  0.906357         0         0         0         1          1.652806   \n",
       " 423 -1.363523         0         0         0         1         -0.676863   \n",
       " 536  1.451128         1         0         0         0          1.361597   \n",
       " 447 -0.273981         1         0         0         0         -0.094446   \n",
       " 382  0.543176         0         0         0         1         -1.259281   \n",
       " 635  1.360333         1         0         0         0         -0.968072   \n",
       " 157  1.087947         0         0         1         0         -0.968072   \n",
       " 647 -0.183185         0         0         0         1         -0.968072   \n",
       " 471  0.906357         1         0         0         0          0.196763   \n",
       " 210 -0.364776         0         0         0         1         -0.385655   \n",
       " 527 -1.091137         0         0         0         1          1.070388   \n",
       " 349  1.451128         0         0         0         1          1.652806   \n",
       " 85  -0.364776         1         0         0         0          1.361597   \n",
       " 575 -0.092390         0         0         0         1         -1.550489   \n",
       " 209  0.815562         1         0         0         0         -0.385655   \n",
       " 466  0.179995         0         0         0         1          0.196763   \n",
       " 538  0.633971         1         0         0         0          1.361597   \n",
       " 566  0.906357         0         0         0         1          1.652806   \n",
       " 720  1.451128         0         0         0         1         -0.094446   \n",
       " 569 -0.364776         0         0         1         0          1.652806   \n",
       " 344  0.179995         0         0         0         1          1.652806   \n",
       " 146  0.906357         0         0         0         1         -1.259281   \n",
       " 104  1.451128         0         0         1         0          1.652806   \n",
       " 116 -0.364776         0         0         1         0         -1.550489   \n",
       " ..        ...       ...       ...       ...       ...               ...   \n",
       " 438  1.451128         0         0         0         1         -0.385655   \n",
       " 81   0.906357         0         0         0         1          1.361597   \n",
       " 725 -1.181933         0         0         0         1          0.196763   \n",
       " 311  0.179995         0         0         0         0          1.070388   \n",
       " 256  0.724767         1         0         0         0          0.487971   \n",
       " 364 -0.273981         1         0         0         0         -1.550489   \n",
       " 208  0.906357         0         0         1         0         -0.385655   \n",
       " 227  0.543176         1         0         0         0         -0.094446   \n",
       " 64   1.632719         0         0         0         0          1.070388   \n",
       " 390  1.451128         0         0         1         0         -0.968072   \n",
       " 252 -0.637161         0         0         1         0          0.487971   \n",
       " 40   0.815562         0         0         0         1          0.779180   \n",
       " 158 -0.092390         0         1         0         0         -0.968072   \n",
       " 97  -0.273981         0         0         0         1          1.361597   \n",
       " 126  1.451128         0         0         0         1         -1.550489   \n",
       " 350 -1.545113         0         0         0         1          1.652806   \n",
       " 653 -0.455571         0         0         0         1         -0.968072   \n",
       " 736 -1.545113         1         0         0         0          0.196763   \n",
       " 660 -0.455571         1         0         0         0         -0.676863   \n",
       " 670 -0.364776         0         0         0         1         -0.676863   \n",
       " 230  1.632719         0         0         0         1         -0.094446   \n",
       " 340 -1.363523         0         0         0         1          1.652806   \n",
       " 580  0.361586         0         0         0         1         -1.550489   \n",
       " 619  1.360333         0         0         0         1         -0.968072   \n",
       " 221 -0.727957         0         0         0         1         -0.094446   \n",
       " 11   0.179995         1         0         0         0          0.196763   \n",
       " 276 -0.909547         0         0         0         0          0.779180   \n",
       " 552  0.906357         0         0         0         1          1.361597   \n",
       " 117  0.906357         0         0         0         1         -1.550489   \n",
       " 239  1.451128         0         0         0         1          0.196763   \n",
       " \n",
       "      Day of the week  Transportation expense  Distance from Residence to Work  \\\n",
       " 87         -1.347819                1.065291                         0.092328   \n",
       " 719        -1.347819               -0.629095                        -0.244901   \n",
       " 26         -0.643947                1.035034                         0.429556   \n",
       " 188         0.763796               -0.992178                        -1.189139   \n",
       " 31          0.763796                0.218098                        -0.042564   \n",
       " 93          1.467667                0.596309                         1.373794   \n",
       " 102         0.763796                0.066813                        -0.244901   \n",
       " 423        -1.347819               -0.629095                         1.441240   \n",
       " 536        -0.643947               -1.551930                        -1.324030   \n",
       " 447        -1.347819                1.065291                         0.092328   \n",
       " 382        -0.643947                0.384511                        -0.312346   \n",
       " 635        -1.347819                0.414767                        -0.312346   \n",
       " 157        -0.643947               -0.961921                        -0.177455   \n",
       " 647         0.059924               -1.551930                        -0.986802   \n",
       " 471        -0.643947                0.066813                        -0.244901   \n",
       " 210        -1.347819               -0.992178                        -1.189139   \n",
       " 527        -1.347819               -0.477811                        -0.042564   \n",
       " 349        -0.643947               -1.551930                        -1.324030   \n",
       " 85         -1.347819               -0.992178                        -1.189139   \n",
       " 575         0.059924               -0.629095                        -0.514683   \n",
       " 209         0.059924               -0.553453                         0.834229   \n",
       " 466        -1.347819                0.596309                         1.373794   \n",
       " 538         1.467667                0.218098                        -0.919357   \n",
       " 566        -0.643947                0.066813                        -0.244901   \n",
       " 720        -1.347819               -1.551930                        -1.324030   \n",
       " 569        -0.643947               -0.992178                        -1.189139   \n",
       " 344         1.467667                0.596309                         1.373794   \n",
       " 146        -1.347819                0.066813                        -0.244901   \n",
       " 104        -0.643947               -1.551930                        -1.324030   \n",
       " 116        -0.643947               -0.992178                        -1.189139   \n",
       " ..               ...                     ...                              ...   \n",
       " 438        -1.347819               -1.551930                        -1.324030   \n",
       " 81          0.059924                0.066813                        -0.244901   \n",
       " 725         0.059924                0.218098                        -0.649574   \n",
       " 311        -0.643947                0.596309                         1.373794   \n",
       " 256         0.763796                1.201447                        -0.244901   \n",
       " 364         0.059924                1.065291                         0.092328   \n",
       " 208        -0.643947                0.066813                        -0.244901   \n",
       " 227        -1.347819                0.384511                        -0.312346   \n",
       " 64          0.059924               -1.551930                        -1.121694   \n",
       " 390        -0.643947               -1.551930                        -1.324030   \n",
       " 252         1.467667                1.035034                         0.429556   \n",
       " 40         -0.643947               -0.553453                         0.834229   \n",
       " 158        -0.643947               -0.629095                        -0.514683   \n",
       " 97          0.763796                1.065291                         0.092328   \n",
       " 126         1.467667               -1.551930                        -1.324030   \n",
       " 350         0.763796                0.218098                        -1.256585   \n",
       " 653         0.059924                2.245309                        -0.851911   \n",
       " 736        -0.643947                0.218098                        -1.256585   \n",
       " 660        -1.347819                2.245309                        -0.851911   \n",
       " 670        -0.643947               -0.992178                        -1.189139   \n",
       " 230        -0.643947               -1.551930                        -1.121694   \n",
       " 340        -1.347819               -0.629095                         1.441240   \n",
       " 580         0.059924               -0.629095                        -0.244901   \n",
       " 619        -1.347819                0.414767                        -0.312346   \n",
       " 221        -1.347819                2.124282                         1.508686   \n",
       " 11         -0.643947                0.596309                         1.373794   \n",
       " 276        -0.643947                0.157584                         0.362110   \n",
       " 552         1.467667                0.066813                        -0.244901   \n",
       " 117         0.059924                0.066813                        -0.244901   \n",
       " 239         1.467667               -1.551930                        -1.324030   \n",
       " \n",
       "      Service time  ...  Work load Average/day   Hit target  \\\n",
       " 87      -0.107020  ...                1.223870   -0.619760   \n",
       " 719     -0.826975  ...                0.249658    0.352573   \n",
       " 26       0.132965  ...               -1.906348   -0.943871   \n",
       " 188      0.372950  ...                1.850580    0.352573   \n",
       " 31      -0.107020  ...               -1.906348   -0.943871   \n",
       " 93      -0.347005  ...                1.223870   -0.619760   \n",
       " 102     -0.826975  ...               -0.179941    0.676684   \n",
       " 423      1.332889  ...               -0.862443    1.000795   \n",
       " 536     -0.586990  ...                0.044880   -0.619760   \n",
       " 447     -0.107020  ...               -0.409000    0.028462   \n",
       " 382      0.852919  ...               -0.475670    0.352573   \n",
       " 635      0.372950  ...               -1.398951    1.324906   \n",
       " 157     -1.546930  ...                2.374247    0.028462   \n",
       " 647      2.772799  ...               -1.398951    1.324906   \n",
       " 471     -0.826975  ...               -1.146671   -0.943871   \n",
       " 210      0.372950  ...               -0.053096   -0.943871   \n",
       " 527      0.132965  ...                0.553990   -1.267983   \n",
       " 349     -0.586990  ...               -0.949093   -0.619760   \n",
       " 85       0.372950  ...                1.223870   -0.619760   \n",
       " 575      1.092904  ...                1.447880    0.352573   \n",
       " 209     -1.306945  ...                0.008953   -0.943871   \n",
       " 466     -0.347005  ...               -1.146671   -0.943871   \n",
       " 538     -1.066960  ...                0.044880   -0.619760   \n",
       " 566     -0.826975  ...                0.419840    1.000795   \n",
       " 720     -0.586990  ...                0.249658    0.352573   \n",
       " 569      0.372950  ...                0.419840    1.000795   \n",
       " 344     -0.347005  ...               -0.949093   -0.619760   \n",
       " 146     -0.826975  ...                1.106675    1.324906   \n",
       " 104     -0.586990  ...               -0.179941    0.676684   \n",
       " 116      0.372950  ...                1.293937    0.028462   \n",
       " ..            ...  ...                     ...         ...   \n",
       " 438     -0.586990  ...               -0.654704    1.324906   \n",
       " 81      -0.826975  ...                1.223870   -0.619760   \n",
       " 725      0.132965  ...               -0.077146   -0.619760   \n",
       " 311     -0.347005  ...               -0.064273   -2.240316   \n",
       " 256      0.132965  ...               -0.045635   -0.295649   \n",
       " 364     -0.107020  ...                1.963068    1.649017   \n",
       " 208     -0.826975  ...               -0.593663   -0.943871   \n",
       " 227      0.852919  ...                0.941136   -0.295649   \n",
       " 64       1.332889  ...               -0.424335   -0.619760   \n",
       " 390     -0.586990  ...               -0.707285    1.000795   \n",
       " 252      0.132965  ...               -0.045635   -0.295649   \n",
       " 40      -1.306945  ...               -0.798018   -0.943871   \n",
       " 158      1.092904  ...                2.374247    0.028462   \n",
       " 97      -0.107020  ...                1.223870   -0.619760   \n",
       " 126     -0.586990  ...                1.293937    0.028462   \n",
       " 350      0.372950  ...               -0.949093   -0.619760   \n",
       " 653     -0.107020  ...               -1.398951    1.324906   \n",
       " 736      0.372950  ...               -0.077146   -0.619760   \n",
       " 660     -0.107020  ...               -0.648033   -1.267983   \n",
       " 670      0.372950  ...               -0.648033   -1.267983   \n",
       " 230      1.332889  ...                0.297177   -0.295649   \n",
       " 340      1.332889  ...               -0.949093   -0.619760   \n",
       " 580     -0.826975  ...                1.447880    0.352573   \n",
       " 619      0.372950  ...               -1.398951    1.324906   \n",
       " 221     -2.266884  ...               -0.433205   -0.295649   \n",
       " 11      -0.347005  ...               -0.857924    0.676684   \n",
       " 276      0.372950  ...                0.845855   -0.025598   \n",
       " 552     -0.826975  ...                0.044880   -0.619760   \n",
       " 117     -0.826975  ...                1.293937    0.028462   \n",
       " 239     -0.586990  ...                0.256609    1.000795   \n",
       " \n",
       "      Disciplinary failure  Education       Son  Social drinker  Social smoker  \\\n",
       " 87              -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 719             -0.239046          1 -0.928191       -1.145644      -0.280566   \n",
       " 26              -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 188             -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 31              -0.239046          0 -0.017234       -1.145644       3.564226   \n",
       " 93              -0.239046          0  2.715637        0.872872      -0.280566   \n",
       " 102             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 423             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " 536             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 447             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 382             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " 635             -0.239046          0  0.893723       -1.145644      -0.280566   \n",
       " 157             -0.239046          0 -0.928191        0.872872       3.564226   \n",
       " 647             -0.239046          0  0.893723        0.872872       3.564226   \n",
       " 471             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 210             -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 527             -0.239046          0  0.893723       -1.145644      -0.280566   \n",
       " 349             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 85              -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 575             -0.239046          1  0.893723       -1.145644       3.564226   \n",
       " 209             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 466             -0.239046          0  2.715637        0.872872      -0.280566   \n",
       " 538             -0.239046          1 -0.928191       -1.145644      -0.280566   \n",
       " 566             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 720             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 569             -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 344             -0.239046          0  2.715637        0.872872      -0.280566   \n",
       " 146             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 104             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 116             -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " ..                    ...        ...       ...             ...            ...   \n",
       " 438             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 81              -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 725             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 311              4.183300          0  2.715637        0.872872      -0.280566   \n",
       " 256             -0.239046          0  0.893723        0.872872       3.564226   \n",
       " 364             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 208             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 227             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " 64               4.183300          0 -0.017234        0.872872      -0.280566   \n",
       " 390             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 252             -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 40              -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 158             -0.239046          1  0.893723       -1.145644       3.564226   \n",
       " 97              -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 126             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 350             -0.239046          1 -0.017234       -1.145644      -0.280566   \n",
       " 653             -0.239046          0  1.804680        0.872872      -0.280566   \n",
       " 736             -0.239046          1 -0.017234       -1.145644      -0.280566   \n",
       " 660             -0.239046          0  1.804680        0.872872      -0.280566   \n",
       " 670             -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 230             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 340             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " 580             -0.239046          1 -0.928191       -1.145644      -0.280566   \n",
       " 619             -0.239046          0  0.893723       -1.145644      -0.280566   \n",
       " 221             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 11              -0.239046          0  2.715637        0.872872      -0.280566   \n",
       " 276              4.183300          0  0.893723        0.872872      -0.280566   \n",
       " 552             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 117             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 239             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " \n",
       "           Pet    Weight  Body mass index  \n",
       " 87   0.192850 -0.468766        -0.391595  \n",
       " 719 -0.566240 -1.789206        -1.792627  \n",
       " 26   0.192850  0.851673         0.775932  \n",
       " 188 -0.566240  1.240037        -0.391595  \n",
       " 31   3.229209  0.696327         1.476449  \n",
       " 93  -0.566240 -1.090150        -0.858606  \n",
       " 102  0.951940 -0.779458        -0.625100  \n",
       " 423 -0.566240  0.774000         1.009438  \n",
       " 536 -0.566240  0.307963         0.308921  \n",
       " 447  0.192850 -0.468766        -0.391595  \n",
       " 382 -0.566240 -0.934804        -0.858606  \n",
       " 635  0.192850  0.540981         1.242943  \n",
       " 157 -0.566240 -0.313421        -1.092111  \n",
       " 647 -0.566240 -0.313421        -0.391595  \n",
       " 471  0.951940 -0.779458        -0.625100  \n",
       " 210 -0.566240  1.240037        -0.391595  \n",
       " 527  0.951940 -0.779458        -0.391595  \n",
       " 349 -0.566240  0.307963         0.308921  \n",
       " 85  -0.566240  1.240037        -0.391595  \n",
       " 575 -0.566240 -1.245495        -1.092111  \n",
       " 209 -0.566240 -1.633860        -1.325617  \n",
       " 466 -0.566240 -1.090150        -0.858606  \n",
       " 538 -0.566240 -0.313421        -0.391595  \n",
       " 566  0.951940 -0.779458        -0.625100  \n",
       " 720 -0.566240  0.307963         0.308921  \n",
       " 569 -0.566240  1.240037        -0.391595  \n",
       " 344 -0.566240 -1.090150        -0.858606  \n",
       " 146  0.951940 -0.779458        -0.625100  \n",
       " 104 -0.566240  0.307963         0.308921  \n",
       " 116 -0.566240  1.240037        -0.391595  \n",
       " ..        ...       ...              ...  \n",
       " 438 -0.566240  0.307963         0.308921  \n",
       " 81   0.951940 -0.779458        -0.625100  \n",
       " 725 -0.566240  2.094439         2.643976  \n",
       " 311 -0.566240 -1.090150        -0.858606  \n",
       " 256  0.192850 -0.158075        -0.391595  \n",
       " 364  0.192850 -0.468766        -0.391595  \n",
       " 208  0.951940 -0.779458        -0.625100  \n",
       " 227 -0.566240 -0.934804        -0.858606  \n",
       " 64  -0.566240  1.473056         1.009438  \n",
       " 390 -0.566240  0.307963         0.308921  \n",
       " 252  0.192850  0.851673         0.775932  \n",
       " 40  -0.566240 -1.633860        -1.325617  \n",
       " 158 -0.566240 -1.245495        -1.092111  \n",
       " 97   0.192850 -0.468766        -0.391595  \n",
       " 126 -0.566240  0.307963         0.308921  \n",
       " 350  0.192850  0.696327         0.542427  \n",
       " 653 -0.566240 -0.701785        -0.391595  \n",
       " 736  0.192850  0.696327         0.542427  \n",
       " 660 -0.566240 -0.701785        -0.391595  \n",
       " 670 -0.566240  1.240037        -0.391595  \n",
       " 230 -0.566240  1.473056         1.009438  \n",
       " 340 -0.566240  0.774000         1.009438  \n",
       " 580 -0.566240 -1.789206        -1.792627  \n",
       " 619  0.192850  0.540981         1.242943  \n",
       " 221  2.470119  0.074944         0.075416  \n",
       " 11  -0.566240 -1.090150        -0.858606  \n",
       " 276  0.951940  1.628402         1.943459  \n",
       " 552  0.951940 -0.779458        -0.625100  \n",
       " 117  0.951940 -0.779458        -0.625100  \n",
       " 239 -0.566240  0.307963         0.308921  \n",
       " \n",
       " [555 rows x 21 columns],\n",
       "            ID  Reason_1  Reason_2  Reason_3  Reason_4  Month of absence  \\\n",
       " 701 -0.727957         1         0         0         0         -0.385655   \n",
       " 520 -1.363523         0         0         0         1          1.070388   \n",
       " 54  -0.637161         0         0         0         0          0.779180   \n",
       " 434 -0.818752         0         0         1         0         -0.385655   \n",
       " 316  1.632719         0         0         0         1          1.070388   \n",
       " 61   0.179995         0         0         0         1          0.779180   \n",
       " 703 -0.092390         0         0         0         1         -0.385655   \n",
       " 272 -0.637161         0         0         1         0          0.779180   \n",
       " 644  0.361586         0         0         0         1         -0.968072   \n",
       " 639 -0.637161         0         0         0         1         -0.968072   \n",
       " 71  -0.273981         0         0         0         1          1.070388   \n",
       " 296  1.451128         1         0         0         0          1.070388   \n",
       " 300 -1.181933         0         0         0         0          1.070388   \n",
       " 568  0.906357         0         0         0         1          1.652806   \n",
       " 12   0.179995         1         0         0         0          0.196763   \n",
       " 688  1.632719         0         0         0         0         -0.385655   \n",
       " 583 -1.363523         0         0         0         1         -1.550489   \n",
       " 182  1.360333         1         0         0         0         -0.968072   \n",
       " 726 -0.546366         0         0         1         0          0.196763   \n",
       " 77  -0.637161         0         0         0         1          1.070388   \n",
       " 211 -1.363523         1         0         0         0         -0.385655   \n",
       " 539  0.906357         0         0         0         1          1.361597   \n",
       " 320  0.179995         0         0         0         1          1.361597   \n",
       " 130 -0.637161         0         0         1         0         -1.550489   \n",
       " 4   -0.637161         0         0         0         1          0.196763   \n",
       " 308  0.361586         0         0         0         1          1.070388   \n",
       " 405 -1.545113         0         0         0         0         -0.968072   \n",
       " 394 -1.363523         0         0         0         1         -0.968072   \n",
       " 424  1.451128         1         0         0         0         -0.676863   \n",
       " 368 -1.363523         0         0         0         1         -1.550489   \n",
       " ..        ...       ...       ...       ...       ...               ...   \n",
       " 335  0.179995         0         1         0         0          1.361597   \n",
       " 24  -0.637161         0         0         1         0          0.487971   \n",
       " 327  1.451128         1         0         0         0          1.361597   \n",
       " 41   1.451128         0         0         0         1          0.779180   \n",
       " 729 -0.818752         1         0         0         0          0.196763   \n",
       " 460  0.361586         0         0         0         1          0.196763   \n",
       " 277  0.089200         0         0         0         0          0.779180   \n",
       " 10   0.179995         1         0         0         0          0.196763   \n",
       " 695 -0.092390         1         0         0         0         -0.385655   \n",
       " 299  0.361586         1         0         0         0          1.070388   \n",
       " 739  1.541923         0         0         0         0         -1.841698   \n",
       " 197  0.179995         0         0         1         0         -0.676863   \n",
       " 343  1.632719         0         0         0         1          1.652806   \n",
       " 59   1.360333         0         0         0         1          0.779180   \n",
       " 646 -1.363523         0         0         0         1         -0.968072   \n",
       " 514 -0.273981         0         0         0         1          1.070388   \n",
       " 395 -1.545113         0         0         0         1         -0.968072   \n",
       " 713  0.361586         0         0         0         1         -0.094446   \n",
       " 141 -1.091137         0         0         0         1         -1.259281   \n",
       " 403  1.632719         1         0         0         0         -0.968072   \n",
       " 133  1.451128         0         0         0         1         -1.550489   \n",
       " 692 -0.364776         1         0         0         0         -0.385655   \n",
       " 705  0.906357         1         0         0         0         -0.385655   \n",
       " 508  0.633971         1         0         0         0          1.070388   \n",
       " 281 -0.273981         0         0         0         1          0.779180   \n",
       " 195 -0.273981         0         0         0         1         -0.676863   \n",
       " 737 -1.272728         0         0         0         0         -1.841698   \n",
       " 280 -1.363523         0         0         0         1          0.779180   \n",
       " 717  0.361586         1         0         0         0         -0.094446   \n",
       " 192  1.632719         0         0         0         1         -0.676863   \n",
       " \n",
       "      Day of the week  Transportation expense  Distance from Residence to Work  \\\n",
       " 701        -1.347819                2.124282                         1.508686   \n",
       " 520         0.059924               -0.629095                         1.441240   \n",
       " 54         -0.643947                1.035034                         0.429556   \n",
       " 434         0.059924                0.112199                        -1.054248   \n",
       " 316         0.763796               -1.551930                        -1.121694   \n",
       " 61          1.467667                0.596309                         1.373794   \n",
       " 703         1.467667               -0.629095                        -0.514683   \n",
       " 272         0.059924                1.035034                         0.429556   \n",
       " 644         1.467667               -0.629095                        -0.244901   \n",
       " 639         0.059924                1.035034                         0.429556   \n",
       " 71          0.763796                1.065291                         0.092328   \n",
       " 296        -1.347819               -1.551930                        -1.324030   \n",
       " 300         0.059924                0.218098                        -0.649574   \n",
       " 568        -1.347819                0.066813                        -0.244901   \n",
       " 12          0.059924                0.596309                         1.373794   \n",
       " 688        -0.643947               -1.551930                        -1.121694   \n",
       " 583         0.763796               -0.629095                         1.441240   \n",
       " 182         1.467667                0.414767                        -0.312346   \n",
       " 726         1.467667                0.187841                         1.441240   \n",
       " 77          0.059924                1.035034                         0.429556   \n",
       " 211        -0.643947               -0.629095                         1.441240   \n",
       " 539         1.467667                0.066813                        -0.244901   \n",
       " 320         1.467667                0.596309                         1.373794   \n",
       " 130        -0.643947                1.035034                         0.429556   \n",
       " 4           0.763796                1.035034                         0.429556   \n",
       " 308         0.763796               -0.629095                        -0.244901   \n",
       " 405         0.763796                0.218098                        -1.256585   \n",
       " 394        -1.347819               -0.629095                         1.441240   \n",
       " 424         0.059924               -1.551930                        -1.324030   \n",
       " 368         1.467667               -0.629095                         1.441240   \n",
       " ..               ...                     ...                              ...   \n",
       " 335         1.467667                0.596309                         1.373794   \n",
       " 24         -1.347819                1.035034                         0.429556   \n",
       " 327         0.059924               -1.551930                        -1.324030   \n",
       " 41         -1.347819               -1.551930                        -1.324030   \n",
       " 729        -0.643947                0.112199                        -1.054248   \n",
       " 460         0.763796               -0.629095                        -0.244901   \n",
       " 277        -0.643947                1.065291                         1.373794   \n",
       " 10         -1.347819                0.596309                         1.373794   \n",
       " 695         0.059924               -0.629095                        -0.514683   \n",
       " 299         0.059924               -0.629095                        -0.244901   \n",
       " 739         1.467667               -0.629095                         1.036566   \n",
       " 197         1.467667                0.596309                         1.373794   \n",
       " 343        -0.643947               -1.551930                        -1.121694   \n",
       " 59          1.467667                0.414767                        -0.312346   \n",
       " 646        -0.643947               -0.629095                         1.441240   \n",
       " 514         1.467667                1.065291                         0.092328   \n",
       " 395        -1.347819                0.218098                        -1.256585   \n",
       " 713         1.467667               -0.629095                        -0.244901   \n",
       " 141         0.763796               -0.477811                        -0.042564   \n",
       " 403        -0.643947               -1.551930                        -1.121694   \n",
       " 133         0.763796               -1.551930                        -1.324030   \n",
       " 692        -1.347819               -0.992178                        -1.189139   \n",
       " 705        -1.347819                0.066813                        -0.244901   \n",
       " 508         0.763796                0.218098                        -0.919357   \n",
       " 281         1.467667                1.065291                         0.092328   \n",
       " 195         1.467667                1.065291                         0.092328   \n",
       " 737        -0.643947               -1.551930                        -1.054248   \n",
       " 280         0.763796               -0.629095                         1.441240   \n",
       " 717         0.763796               -0.629095                        -0.244901   \n",
       " 192         0.763796               -1.551930                        -1.121694   \n",
       " \n",
       "      Service time  ...  Work load Average/day   Hit target  \\\n",
       " 701     -2.266884  ...               -0.917082    1.324906   \n",
       " 520      1.332889  ...                0.553990   -1.267983   \n",
       " 54       0.132965  ...               -0.798018   -0.943871   \n",
       " 434      0.852919  ...               -0.654704    1.324906   \n",
       " 316      1.332889  ...               -0.064273   -2.240316   \n",
       " 61      -0.347005  ...               -0.798018   -0.943871   \n",
       " 703      1.092904  ...               -0.917082    1.324906   \n",
       " 272      0.132965  ...                0.845855   -0.531425   \n",
       " 644     -0.826975  ...               -1.398951    1.324906   \n",
       " 639      0.132965  ...               -1.398951    1.324906   \n",
       " 71      -0.107020  ...               -0.424335   -0.619760   \n",
       " 296     -0.586990  ...               -0.064273   -2.240316   \n",
       " 300      0.132965  ...               -0.064273   -2.240316   \n",
       " 568     -0.826975  ...                0.419840    1.000795   \n",
       " 12      -0.347005  ...               -0.857924    0.676684   \n",
       " 688      1.332889  ...               -0.917082    1.324906   \n",
       " 583      1.332889  ...                1.447880    0.352573   \n",
       " 182      0.372950  ...                2.374247    0.028462   \n",
       " 726     -2.746854  ...               -0.077146   -0.619760   \n",
       " 77       0.132965  ...               -0.424335   -0.619760   \n",
       " 211      1.332889  ...                0.811686   -0.943871   \n",
       " 539     -0.826975  ...                0.044880   -0.619760   \n",
       " 320     -0.347005  ...                0.528370    0.676684   \n",
       " 130      0.132965  ...                1.293937    0.028462   \n",
       " 4        0.132965  ...               -0.857924    0.676684   \n",
       " 308     -0.826975  ...               -0.064273   -2.240316   \n",
       " 405      0.372950  ...               -0.707285    1.000795   \n",
       " 394      1.332889  ...               -0.707285    1.000795   \n",
       " 424     -0.586990  ...               -0.862443    1.000795   \n",
       " 368      1.332889  ...                1.963068    1.649017   \n",
       " ..            ...  ...                     ...         ...   \n",
       " 335     -0.347005  ...                0.528370    0.676684   \n",
       " 24       0.132965  ...               -1.906348   -0.943871   \n",
       " 327     -0.586990  ...                0.528370    0.676684   \n",
       " 41      -0.586990  ...               -0.798018   -0.943871   \n",
       " 729      0.852919  ...               -0.077146   -0.619760   \n",
       " 460     -0.826975  ...               -1.146671   -0.943871   \n",
       " 277     -0.107020  ...                0.845855   -0.028259   \n",
       " 10      -0.347005  ...               -0.857924    0.676684   \n",
       " 695      1.092904  ...               -0.917082    1.324906   \n",
       " 299     -0.826975  ...               -0.064273   -2.240316   \n",
       " 739      0.372950  ...                0.129035    0.028462   \n",
       " 197     -0.347005  ...                1.850580    0.352573   \n",
       " 343      1.332889  ...               -0.949093   -0.619760   \n",
       " 59       0.372950  ...               -0.798018   -0.943871   \n",
       " 646      1.332889  ...               -1.398951    1.324906   \n",
       " 514     -0.107020  ...                0.553990   -1.267983   \n",
       " 395      0.372950  ...               -0.707285    1.000795   \n",
       " 713     -0.826975  ...                0.249658    0.352573   \n",
       " 141      0.132965  ...                1.106675    1.324906   \n",
       " 403      1.332889  ...               -0.707285    1.000795   \n",
       " 133     -0.586990  ...                1.293937    0.028462   \n",
       " 692      0.372950  ...               -0.917082    1.324906   \n",
       " 705     -0.826975  ...               -0.917082    1.324906   \n",
       " 508     -1.066960  ...                0.553990   -1.267983   \n",
       " 281     -0.107020  ...                0.845855   -0.696087   \n",
       " 195     -0.107020  ...                1.850580    0.352573   \n",
       " 737      0.132965  ...                0.129035    0.028462   \n",
       " 280      1.332889  ...                0.845855   -0.509172   \n",
       " 717     -0.826975  ...                0.249658    0.352573   \n",
       " 192      1.332889  ...                1.850580    0.352573   \n",
       " \n",
       "      Disciplinary failure  Education       Son  Social drinker  Social smoker  \\\n",
       " 701             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 520             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " 54               4.183300          0  0.893723        0.872872      -0.280566   \n",
       " 434             -0.239046          0  0.893723       -1.145644      -0.280566   \n",
       " 316             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 61              -0.239046          0  2.715637        0.872872      -0.280566   \n",
       " 703             -0.239046          1  0.893723       -1.145644       3.564226   \n",
       " 272             -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 644             -0.239046          1 -0.928191       -1.145644      -0.280566   \n",
       " 639             -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 71              -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 296             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 300              4.183300          0 -0.017234        0.872872      -0.280566   \n",
       " 568             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 12              -0.239046          0  2.715637        0.872872      -0.280566   \n",
       " 688              4.183300          0 -0.017234        0.872872      -0.280566   \n",
       " 583             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " 182             -0.239046          0  0.893723       -1.145644      -0.280566   \n",
       " 726             -0.239046          1 -0.017234        0.872872      -0.280566   \n",
       " 77              -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 211             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " 539             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 320             -0.239046          0  2.715637        0.872872      -0.280566   \n",
       " 130             -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 4               -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 308             -0.239046          1 -0.928191       -1.145644      -0.280566   \n",
       " 405              4.183300          1 -0.017234       -1.145644      -0.280566   \n",
       " 394             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " 424             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 368             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " ..                    ...        ...       ...             ...            ...   \n",
       " 335             -0.239046          0  2.715637        0.872872      -0.280566   \n",
       " 24              -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 327             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 41              -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 729             -0.239046          0  0.893723       -1.145644      -0.280566   \n",
       " 460             -0.239046          1 -0.928191       -1.145644      -0.280566   \n",
       " 277              4.183300          0 -0.928191        0.872872      -0.280566   \n",
       " 10              -0.239046          0  2.715637        0.872872      -0.280566   \n",
       " 695             -0.239046          1  0.893723       -1.145644       3.564226   \n",
       " 299             -0.239046          1 -0.928191       -1.145644      -0.280566   \n",
       " 739             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 197             -0.239046          0  2.715637        0.872872      -0.280566   \n",
       " 343             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 59              -0.239046          0  0.893723       -1.145644      -0.280566   \n",
       " 646             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " 514             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 395             -0.239046          1 -0.017234       -1.145644      -0.280566   \n",
       " 713             -0.239046          1 -0.928191       -1.145644      -0.280566   \n",
       " 141             -0.239046          0  0.893723       -1.145644      -0.280566   \n",
       " 403             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 133             -0.239046          0 -0.928191       -1.145644      -0.280566   \n",
       " 692             -0.239046          0  0.893723        0.872872      -0.280566   \n",
       " 705             -0.239046          0 -0.017234       -1.145644      -0.280566   \n",
       " 508             -0.239046          1 -0.928191       -1.145644      -0.280566   \n",
       " 281             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 195             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 737             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " 280             -0.239046          0 -0.928191        0.872872      -0.280566   \n",
       " 717             -0.239046          1 -0.928191       -1.145644      -0.280566   \n",
       " 192             -0.239046          0 -0.017234        0.872872      -0.280566   \n",
       " \n",
       "           Pet    Weight  Body mass index  \n",
       " 701  2.470119  0.074944         0.075416  \n",
       " 520 -0.566240  0.774000         1.009438  \n",
       " 54   0.192850  0.851673         0.775932  \n",
       " 434  0.192850 -1.090150        -1.092111  \n",
       " 316 -0.566240  1.473056         1.009438  \n",
       " 61  -0.566240 -1.090150        -0.858606  \n",
       " 703 -0.566240 -1.245495        -1.092111  \n",
       " 272  0.192850  0.851673         0.775932  \n",
       " 644 -0.566240 -1.789206        -1.792627  \n",
       " 639  0.192850  0.851673         0.775932  \n",
       " 71   0.192850 -0.468766        -0.391595  \n",
       " 296 -0.566240  0.307963         0.308921  \n",
       " 300 -0.566240  2.094439         2.643976  \n",
       " 568  0.951940 -0.779458        -0.625100  \n",
       " 12  -0.566240 -1.090150        -0.858606  \n",
       " 688 -0.566240  1.473056         1.009438  \n",
       " 583 -0.566240  0.774000         1.009438  \n",
       " 182  0.192850  0.540981         1.242943  \n",
       " 726  5.506478 -0.857131        -1.325617  \n",
       " 77   0.192850  0.851673         0.775932  \n",
       " 211 -0.566240  0.774000         1.009438  \n",
       " 539  0.951940 -0.779458        -0.625100  \n",
       " 320 -0.566240 -1.090150        -0.858606  \n",
       " 130  0.192850  0.851673         0.775932  \n",
       " 4    0.192850  0.851673         0.775932  \n",
       " 308 -0.566240 -1.789206        -1.792627  \n",
       " 405  0.192850  0.696327         0.542427  \n",
       " 394 -0.566240  0.774000         1.009438  \n",
       " 424 -0.566240  0.307963         0.308921  \n",
       " 368 -0.566240  0.774000         1.009438  \n",
       " ..        ...       ...              ...  \n",
       " 335 -0.566240 -1.090150        -0.858606  \n",
       " 24   0.192850  0.851673         0.775932  \n",
       " 327 -0.566240  0.307963         0.308921  \n",
       " 41  -0.566240  0.307963         0.308921  \n",
       " 729  0.192850 -1.090150        -1.092111  \n",
       " 460 -0.566240 -1.789206        -1.792627  \n",
       " 277 -0.566240 -1.090150        -0.858606  \n",
       " 10  -0.566240 -1.090150        -0.858606  \n",
       " 695 -0.566240 -1.245495        -1.092111  \n",
       " 299 -0.566240 -1.789206        -1.792627  \n",
       " 739  0.192850 -0.158075        -0.391595  \n",
       " 197 -0.566240 -1.090150        -0.858606  \n",
       " 343 -0.566240  1.473056         1.009438  \n",
       " 59   0.192850  0.540981         1.242943  \n",
       " 646 -0.566240  0.774000         1.009438  \n",
       " 514  0.192850 -0.468766        -0.391595  \n",
       " 395  0.192850  0.696327         0.542427  \n",
       " 713 -0.566240 -1.789206        -1.792627  \n",
       " 141  0.951940 -0.779458        -0.391595  \n",
       " 403 -0.566240  1.473056         1.009438  \n",
       " 133 -0.566240  0.307963         0.308921  \n",
       " 692 -0.566240  1.240037        -0.391595  \n",
       " 705  0.951940 -0.779458        -0.625100  \n",
       " 508 -0.566240 -0.313421        -0.391595  \n",
       " 281  0.192850 -0.468766        -0.391595  \n",
       " 195  0.192850 -0.468766        -0.391595  \n",
       " 737  5.506478  1.473056         1.709954  \n",
       " 280 -0.566240  0.774000         1.009438  \n",
       " 717 -0.566240 -1.789206        -1.792627  \n",
       " 192 -0.566240  1.473056         1.009438  \n",
       " \n",
       " [185 rows x 21 columns],\n",
       " array([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1]),\n",
       " array([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split(scaled_inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test =train_test_split(scaled_inputs,targets,train_size=0.8,random_state=20)\n",
    "# Here we used 80-20 split to train the model on less data\n",
    "# we use shuffle method, shuffle is boolean deafult shuffle is true\n",
    "# we use random_state to make the shuffle psudo random shuffling will take place in same random way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592, 21) (592,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 21) (148,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this input contains 592 observations along 19 feature\n",
    "#while targets are vector of length 592"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model and analysing its accuracy for diffrent machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy For SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SVM: 0.7297297297297297\n"
     ]
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy for SVM:\",metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy For Gaussian Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Naive-Bayes:  0.43243243243243246\n"
     ]
    }
   ],
   "source": [
    "# import the necessary module\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#create an object of the type GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#train the algorithm on training data and predict using the testing data\n",
    "pred = gnb.fit(x_train, y_train).predict(x_test)\n",
    "#print(pred.tolist())\n",
    "\n",
    "#print the accuracy score of the model\n",
    "print(\"Accuracy for Naive-Bayes: \",accuracy_score(y_test, pred, normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy For Decision Tree Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Decision Tree Classifier: 0.6891891891891891\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy for Decision Tree Classifier:\",accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for RandomForestClassifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7635135135135135\n"
     ]
    }
   ],
   "source": [
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "Model=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "Model.fit(x_train,y_train)\n",
    "\n",
    "y_pred=Model.predict(x_test)\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy for Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy for Logistic Regression : 0.7635135135135135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "reg=LogisticRegression()\n",
    "print(reg.fit(x_train,y_train))\n",
    "y_pred1=Model.predict(x_test)\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy for Logistic Regression :\",metrics.accuracy_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ So, from above observations we see that Random forest and Logistic regression have good accuracys as compared to others.\n",
    "\n",
    "+ Prior to updation of dataset we got more accuracy for Logistic Regression model as compared to Random Forest.There was only +2 to +-3 % diffrence in both models.So due to this reason we kept our implementation in Logistic Regression model and didnt shift to Random Forest because of time constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hiesenberg\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(x_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7516891891891891"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To cross check the accuracy we manually check the accuracy as belows:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs=reg.predict(x_train)\n",
    "model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "       False,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True, False, False,  True,\n",
       "        True, False,  True,  True,  True, False,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True, False,  True, False,  True, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True, False,  True, False,\n",
       "        True, False,  True,  True,  True, False, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "       False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True, False,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        True,  True,  True,  True, False,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False, False,  True,  True,\n",
       "       False, False,  True, False,  True, False,  True,  True, False,\n",
       "        True,  True,  True, False,  True,  True,  True,  True, False,\n",
       "       False,  True,  True, False,  True, False,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "       False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "       False, False,  True,  True, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False,  True, False,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False,  True,  True,\n",
       "        True, False,  True, False, False,  True, False,  True,  True,\n",
       "        True, False,  True,  True,  True, False,  True,  True, False,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False, False,  True,  True, False,  True,  True,\n",
       "        True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        True, False,  True, False, False, False,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False,  True, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "       False,  True, False,  True, False,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True, False, False,  True, False,  True,\n",
       "        True, False,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True, False, False,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True, False,  True, False,  True,  True,\n",
       "        True,  True, False,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True, False, False, False,  True, False,  True,\n",
       "        True,  True,  True,  True,  True, False, False,  True, False,\n",
       "        True,  True, False,  True, False,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "        True, False,  True,  True, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True, False, False, False, False,  True, False,\n",
       "       False,  True,  True,  True, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False, False,  True,  True, False,  True,\n",
       "        True,  True,  True,  True,  True,  True, False])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs==y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model_outputs==y_train) # to count the correctly predicted outputs\n",
    "#below is total number of true elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7516891891891891"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model_outputs==y_train)/model_outputs.shape[0] # same result as sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00181233])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To find intercept\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.63541489,  1.02676777, -0.02323279,  1.26745927, -0.9014043 ,\n",
       "         0.08329575, -0.17809823,  0.41949542, -0.21752706, -0.69143869,\n",
       "         0.40762142, -0.12800716, -0.04099322, -1.3511442 , -0.65818296,\n",
       "         0.41791297,  0.17451871,  0.11020753, -0.45360169,  0.08831005,\n",
       "        -0.13885486]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find out coefficients\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_inputs.columns.values\n",
    "feature_names=unscaled_inputs.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature name</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>-0.001812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>-0.635415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reason_1</td>\n",
       "      <td>1.026768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reason_2</td>\n",
       "      <td>-0.023233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reason_3</td>\n",
       "      <td>1.267459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reason_4</td>\n",
       "      <td>-0.901404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Month of absence</td>\n",
       "      <td>0.083296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Day of the week</td>\n",
       "      <td>-0.178098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Transportation expense</td>\n",
       "      <td>0.419495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Distance from Residence to Work</td>\n",
       "      <td>-0.217527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Service time</td>\n",
       "      <td>-0.691439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.407621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Work load Average/day</td>\n",
       "      <td>-0.128007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hit target</td>\n",
       "      <td>-0.040993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Disciplinary failure</td>\n",
       "      <td>-1.351144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Education</td>\n",
       "      <td>-0.658183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Son</td>\n",
       "      <td>0.417913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Social drinker</td>\n",
       "      <td>0.174519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Social smoker</td>\n",
       "      <td>0.110208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pet</td>\n",
       "      <td>-0.453602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Weight</td>\n",
       "      <td>0.088310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Body mass index</td>\n",
       "      <td>-0.138855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature name  Coefficient\n",
       "0                         Intercept    -0.001812\n",
       "1                                ID    -0.635415\n",
       "2                          Reason_1     1.026768\n",
       "3                          Reason_2    -0.023233\n",
       "4                          Reason_3     1.267459\n",
       "5                          Reason_4    -0.901404\n",
       "6                  Month of absence     0.083296\n",
       "7                   Day of the week    -0.178098\n",
       "8            Transportation expense     0.419495\n",
       "9   Distance from Residence to Work    -0.217527\n",
       "10                     Service time    -0.691439\n",
       "11                              Age     0.407621\n",
       "12           Work load Average/day     -0.128007\n",
       "13                       Hit target    -0.040993\n",
       "14             Disciplinary failure    -1.351144\n",
       "15                        Education    -0.658183\n",
       "16                              Son     0.417913\n",
       "17                   Social drinker     0.174519\n",
       "18                    Social smoker     0.110208\n",
       "19                              Pet    -0.453602\n",
       "20                           Weight     0.088310\n",
       "21                  Body mass index    -0.138855"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table=pd.DataFrame(columns=['Feature name'],data=feature_names)\n",
    "\n",
    "summary_table['Coefficient']= np.transpose(reg.coef_) \n",
    "#transposed beacause the output was in nd array and nd arrays are rows not cloumns\n",
    "\n",
    "summary_table\n",
    "\n",
    "summary_table.index=summary_table.index+1\n",
    "summary_table.loc[0]=['Intercept', reg.intercept_[0]]\n",
    "summary_table=summary_table.sort_index()\n",
    "summary_table\n",
    "\n",
    "# coefficients are called weight and bias is called intercept\n",
    "\n",
    "#Interpreting the coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature name</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>-0.001812</td>\n",
       "      <td>0.998189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>-0.635415</td>\n",
       "      <td>0.529716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reason_1</td>\n",
       "      <td>1.026768</td>\n",
       "      <td>2.792027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reason_2</td>\n",
       "      <td>-0.023233</td>\n",
       "      <td>0.977035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reason_3</td>\n",
       "      <td>1.267459</td>\n",
       "      <td>3.551817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reason_4</td>\n",
       "      <td>-0.901404</td>\n",
       "      <td>0.405999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Month of absence</td>\n",
       "      <td>0.083296</td>\n",
       "      <td>1.086863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Day of the week</td>\n",
       "      <td>-0.178098</td>\n",
       "      <td>0.836860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Transportation expense</td>\n",
       "      <td>0.419495</td>\n",
       "      <td>1.521194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Distance from Residence to Work</td>\n",
       "      <td>-0.217527</td>\n",
       "      <td>0.804506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Service time</td>\n",
       "      <td>-0.691439</td>\n",
       "      <td>0.500855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.407621</td>\n",
       "      <td>1.503238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Work load Average/day</td>\n",
       "      <td>-0.128007</td>\n",
       "      <td>0.879847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hit target</td>\n",
       "      <td>-0.040993</td>\n",
       "      <td>0.959836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Disciplinary failure</td>\n",
       "      <td>-1.351144</td>\n",
       "      <td>0.258944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Education</td>\n",
       "      <td>-0.658183</td>\n",
       "      <td>0.517791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Son</td>\n",
       "      <td>0.417913</td>\n",
       "      <td>1.518788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Social drinker</td>\n",
       "      <td>0.174519</td>\n",
       "      <td>1.190673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Social smoker</td>\n",
       "      <td>0.110208</td>\n",
       "      <td>1.116510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pet</td>\n",
       "      <td>-0.453602</td>\n",
       "      <td>0.635336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Weight</td>\n",
       "      <td>0.088310</td>\n",
       "      <td>1.092327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Body mass index</td>\n",
       "      <td>-0.138855</td>\n",
       "      <td>0.870354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature name  Coefficient  Odds_ratio\n",
       "0                         Intercept    -0.001812    0.998189\n",
       "1                                ID    -0.635415    0.529716\n",
       "2                          Reason_1     1.026768    2.792027\n",
       "3                          Reason_2    -0.023233    0.977035\n",
       "4                          Reason_3     1.267459    3.551817\n",
       "5                          Reason_4    -0.901404    0.405999\n",
       "6                  Month of absence     0.083296    1.086863\n",
       "7                   Day of the week    -0.178098    0.836860\n",
       "8            Transportation expense     0.419495    1.521194\n",
       "9   Distance from Residence to Work    -0.217527    0.804506\n",
       "10                     Service time    -0.691439    0.500855\n",
       "11                              Age     0.407621    1.503238\n",
       "12           Work load Average/day     -0.128007    0.879847\n",
       "13                       Hit target    -0.040993    0.959836\n",
       "14             Disciplinary failure    -1.351144    0.258944\n",
       "15                        Education    -0.658183    0.517791\n",
       "16                              Son     0.417913    1.518788\n",
       "17                   Social drinker     0.174519    1.190673\n",
       "18                    Social smoker     0.110208    1.116510\n",
       "19                              Pet    -0.453602    0.635336\n",
       "20                           Weight     0.088310    1.092327\n",
       "21                  Body mass index    -0.138855    0.870354"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table['Odds_ratio']=np.exp(summary_table.Coefficient)\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Whenever we are dealing with Logistic Regression we actaully are predicting coefficients called log odds\n",
    "+ Logistic Regression are linear functions predicting log odds which are separted as 0 nad 1 later\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature name</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reason_3</td>\n",
       "      <td>1.267459</td>\n",
       "      <td>3.551817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reason_1</td>\n",
       "      <td>1.026768</td>\n",
       "      <td>2.792027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Transportation expense</td>\n",
       "      <td>0.419495</td>\n",
       "      <td>1.521194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Son</td>\n",
       "      <td>0.417913</td>\n",
       "      <td>1.518788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.407621</td>\n",
       "      <td>1.503238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Social drinker</td>\n",
       "      <td>0.174519</td>\n",
       "      <td>1.190673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Social smoker</td>\n",
       "      <td>0.110208</td>\n",
       "      <td>1.116510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Weight</td>\n",
       "      <td>0.088310</td>\n",
       "      <td>1.092327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Month of absence</td>\n",
       "      <td>0.083296</td>\n",
       "      <td>1.086863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>-0.001812</td>\n",
       "      <td>0.998189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reason_2</td>\n",
       "      <td>-0.023233</td>\n",
       "      <td>0.977035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hit target</td>\n",
       "      <td>-0.040993</td>\n",
       "      <td>0.959836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Work load Average/day</td>\n",
       "      <td>-0.128007</td>\n",
       "      <td>0.879847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Body mass index</td>\n",
       "      <td>-0.138855</td>\n",
       "      <td>0.870354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Day of the week</td>\n",
       "      <td>-0.178098</td>\n",
       "      <td>0.836860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Distance from Residence to Work</td>\n",
       "      <td>-0.217527</td>\n",
       "      <td>0.804506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pet</td>\n",
       "      <td>-0.453602</td>\n",
       "      <td>0.635336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>-0.635415</td>\n",
       "      <td>0.529716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Education</td>\n",
       "      <td>-0.658183</td>\n",
       "      <td>0.517791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Service time</td>\n",
       "      <td>-0.691439</td>\n",
       "      <td>0.500855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reason_4</td>\n",
       "      <td>-0.901404</td>\n",
       "      <td>0.405999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Disciplinary failure</td>\n",
       "      <td>-1.351144</td>\n",
       "      <td>0.258944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Feature name  Coefficient  Odds_ratio\n",
       "4                          Reason_3     1.267459    3.551817\n",
       "2                          Reason_1     1.026768    2.792027\n",
       "8            Transportation expense     0.419495    1.521194\n",
       "16                              Son     0.417913    1.518788\n",
       "11                              Age     0.407621    1.503238\n",
       "17                   Social drinker     0.174519    1.190673\n",
       "18                    Social smoker     0.110208    1.116510\n",
       "20                           Weight     0.088310    1.092327\n",
       "6                  Month of absence     0.083296    1.086863\n",
       "0                         Intercept    -0.001812    0.998189\n",
       "3                          Reason_2    -0.023233    0.977035\n",
       "13                       Hit target    -0.040993    0.959836\n",
       "12           Work load Average/day     -0.128007    0.879847\n",
       "21                  Body mass index    -0.138855    0.870354\n",
       "7                   Day of the week    -0.178098    0.836860\n",
       "9   Distance from Residence to Work    -0.217527    0.804506\n",
       "19                              Pet    -0.453602    0.635336\n",
       "1                                ID    -0.635415    0.529716\n",
       "15                        Education    -0.658183    0.517791\n",
       "10                     Service time    -0.691439    0.500855\n",
       "5                          Reason_4    -0.901404    0.405999\n",
       "14             Disciplinary failure    -1.351144    0.258944"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table.sort_values('Odds_ratio',ascending=False)\n",
    "# We are assembling it in ascending oreder to hepls eliminate the columns which are of least importance\n",
    "# This the backward elimination method we used for eliminating columns like \"height\", \"Seasons \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Feature is not particularly important:-\n",
    "\n",
    "+ If its coefficient is around 0\n",
    "\n",
    "+ If its odds ratio is around 1\n",
    "\n",
    "A weight(coefficient) of 0 implies that no matter the feature value, we will multiply it by 0(in model)\n",
    "\n",
    "For a unit change in the standardie feature, the odds increase by a multiple equal to the odds ratio(1=no change)\n",
    "\n",
    "eg- if odds is 5:1 and odds ratio is 2 new odds will be product of both i.e 10:1\n",
    "\n",
    "+ We actually used backward elemination method to remove the features which didnt add any value to our dataset. So by referring above we decided to eliminate Days of the week, Height and seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7432432432432432"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We see that the testing accuracy is 74% and training accuracy is 75%. This implies that our model is stable there no overfitting or underfitting occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62748828, 0.37251172],\n",
       "       [0.10209998, 0.89790002],\n",
       "       [0.87771058, 0.12228942],\n",
       "       [0.86508857, 0.13491143],\n",
       "       [0.42882404, 0.57117596],\n",
       "       [0.6925957 , 0.3074043 ],\n",
       "       [0.51255385, 0.48744615],\n",
       "       [0.38984755, 0.61015245],\n",
       "       [0.76280526, 0.23719474],\n",
       "       [0.87666281, 0.12333719],\n",
       "       [0.19292502, 0.80707498],\n",
       "       [0.86245623, 0.13754377],\n",
       "       [0.77548936, 0.22451064],\n",
       "       [0.78662821, 0.21337179],\n",
       "       [0.5802411 , 0.4197589 ],\n",
       "       [0.33108481, 0.66891519],\n",
       "       [0.74699403, 0.25300597],\n",
       "       [0.19675261, 0.80324739],\n",
       "       [0.75406216, 0.24593784],\n",
       "       [0.45851359, 0.54148641],\n",
       "       [0.78624788, 0.21375212],\n",
       "       [0.80691344, 0.19308656],\n",
       "       [0.701383  , 0.298617  ],\n",
       "       [0.87608695, 0.12391305],\n",
       "       [0.30037181, 0.69962819],\n",
       "       [0.37422638, 0.62577362],\n",
       "       [0.18839776, 0.81160224],\n",
       "       [0.54966672, 0.45033328],\n",
       "       [0.44981578, 0.55018422],\n",
       "       [0.54158817, 0.45841183],\n",
       "       [0.91258597, 0.08741403],\n",
       "       [0.33952228, 0.66047772],\n",
       "       [0.03511968, 0.96488032],\n",
       "       [0.0884248 , 0.9115752 ],\n",
       "       [0.44598225, 0.55401775],\n",
       "       [0.88135774, 0.11864226],\n",
       "       [0.6418555 , 0.3581445 ],\n",
       "       [0.64980841, 0.35019159],\n",
       "       [0.86814951, 0.13185049],\n",
       "       [0.06768018, 0.93231982],\n",
       "       [0.01961106, 0.98038894],\n",
       "       [0.83990423, 0.16009577],\n",
       "       [0.06892882, 0.93107118],\n",
       "       [0.39812963, 0.60187037],\n",
       "       [0.23021214, 0.76978786],\n",
       "       [0.70594119, 0.29405881],\n",
       "       [0.68085271, 0.31914729],\n",
       "       [0.37628582, 0.62371418],\n",
       "       [0.82569749, 0.17430251],\n",
       "       [0.63473641, 0.36526359],\n",
       "       [0.84574081, 0.15425919],\n",
       "       [0.33737096, 0.66262904],\n",
       "       [0.98399408, 0.01600592],\n",
       "       [0.30319465, 0.69680535],\n",
       "       [0.39809159, 0.60190841],\n",
       "       [0.4734492 , 0.5265508 ],\n",
       "       [0.87214982, 0.12785018],\n",
       "       [0.15982458, 0.84017542],\n",
       "       [0.15011732, 0.84988268],\n",
       "       [0.03592225, 0.96407775],\n",
       "       [0.77548936, 0.22451064],\n",
       "       [0.70828655, 0.29171345],\n",
       "       [0.40227448, 0.59772552],\n",
       "       [0.89634162, 0.10365838],\n",
       "       [0.84903203, 0.15096797],\n",
       "       [0.70828655, 0.29171345],\n",
       "       [0.30242879, 0.69757121],\n",
       "       [0.36674465, 0.63325535],\n",
       "       [0.05059566, 0.94940434],\n",
       "       [0.89429768, 0.10570232],\n",
       "       [0.44765695, 0.55234305],\n",
       "       [0.77924927, 0.22075073],\n",
       "       [0.39781761, 0.60218239],\n",
       "       [0.95409861, 0.04590139],\n",
       "       [0.77548936, 0.22451064],\n",
       "       [0.33703325, 0.66296675],\n",
       "       [0.31436729, 0.68563271],\n",
       "       [0.03607053, 0.96392947],\n",
       "       [0.7411086 , 0.2588914 ],\n",
       "       [0.42612666, 0.57387334],\n",
       "       [0.09851437, 0.90148563],\n",
       "       [0.87771058, 0.12228942],\n",
       "       [0.29729552, 0.70270448],\n",
       "       [0.42641312, 0.57358688],\n",
       "       [0.07474104, 0.92525896],\n",
       "       [0.87771058, 0.12228942],\n",
       "       [0.08509158, 0.91490842],\n",
       "       [0.05619394, 0.94380606],\n",
       "       [0.65221649, 0.34778351],\n",
       "       [0.78989882, 0.21010118],\n",
       "       [0.41309572, 0.58690428],\n",
       "       [0.81145691, 0.18854309],\n",
       "       [0.31239514, 0.68760486],\n",
       "       [0.05920681, 0.94079319],\n",
       "       [0.85707721, 0.14292279],\n",
       "       [0.45488901, 0.54511099],\n",
       "       [0.47115789, 0.52884211],\n",
       "       [0.80052029, 0.19947971],\n",
       "       [0.3376958 , 0.6623042 ],\n",
       "       [0.79066266, 0.20933734],\n",
       "       [0.85042962, 0.14957038],\n",
       "       [0.88548812, 0.11451188],\n",
       "       [0.80656038, 0.19343962],\n",
       "       [0.90515413, 0.09484587],\n",
       "       [0.39008429, 0.60991571],\n",
       "       [0.77548936, 0.22451064],\n",
       "       [0.72618813, 0.27381187],\n",
       "       [0.5419272 , 0.4580728 ],\n",
       "       [0.9808695 , 0.0191305 ],\n",
       "       [0.83506115, 0.16493885],\n",
       "       [0.52333094, 0.47666906],\n",
       "       [0.11959548, 0.88040452],\n",
       "       [0.9986261 , 0.0013739 ],\n",
       "       [0.99889471, 0.00110529],\n",
       "       [0.2008282 , 0.7991718 ],\n",
       "       [0.75184314, 0.24815686],\n",
       "       [0.27591788, 0.72408212],\n",
       "       [0.47901767, 0.52098233],\n",
       "       [0.0314021 , 0.9685979 ],\n",
       "       [0.17338421, 0.82661579],\n",
       "       [0.46117752, 0.53882248],\n",
       "       [0.42198824, 0.57801176],\n",
       "       [0.81612146, 0.18387854],\n",
       "       [0.38333147, 0.61666853],\n",
       "       [0.83588206, 0.16411794],\n",
       "       [0.51239089, 0.48760911],\n",
       "       [0.99853029, 0.00146971],\n",
       "       [0.44447905, 0.55552095],\n",
       "       [0.72342653, 0.27657347],\n",
       "       [0.90615989, 0.09384011],\n",
       "       [0.64980841, 0.35019159],\n",
       "       [0.29101563, 0.70898437],\n",
       "       [0.84813863, 0.15186137],\n",
       "       [0.60545136, 0.39454864],\n",
       "       [0.97695033, 0.02304967],\n",
       "       [0.23376097, 0.76623903],\n",
       "       [0.79152568, 0.20847432],\n",
       "       [0.85831811, 0.14168189],\n",
       "       [0.27382089, 0.72617911],\n",
       "       [0.79139598, 0.20860402],\n",
       "       [0.44747747, 0.55252253],\n",
       "       [0.55551678, 0.44448322],\n",
       "       [0.50065949, 0.49934051],\n",
       "       [0.69400899, 0.30599101],\n",
       "       [0.31538055, 0.68461945],\n",
       "       [0.73840898, 0.26159102],\n",
       "       [0.89383187, 0.10616813],\n",
       "       [0.98760744, 0.01239256]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the predicted probabilities of each class\n",
    "# the first column shows the probability of a particular observation to be 0, while the second one - to be 1\n",
    "predicted_proba = reg.predict_proba(x_test)\n",
    "\n",
    "# let's check that out\n",
    "predicted_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37251172, 0.89790002, 0.12228942, 0.13491143, 0.57117596,\n",
       "       0.3074043 , 0.48744615, 0.61015245, 0.23719474, 0.12333719,\n",
       "       0.80707498, 0.13754377, 0.22451064, 0.21337179, 0.4197589 ,\n",
       "       0.66891519, 0.25300597, 0.80324739, 0.24593784, 0.54148641,\n",
       "       0.21375212, 0.19308656, 0.298617  , 0.12391305, 0.69962819,\n",
       "       0.62577362, 0.81160224, 0.45033328, 0.55018422, 0.45841183,\n",
       "       0.08741403, 0.66047772, 0.96488032, 0.9115752 , 0.55401775,\n",
       "       0.11864226, 0.3581445 , 0.35019159, 0.13185049, 0.93231982,\n",
       "       0.98038894, 0.16009577, 0.93107118, 0.60187037, 0.76978786,\n",
       "       0.29405881, 0.31914729, 0.62371418, 0.17430251, 0.36526359,\n",
       "       0.15425919, 0.66262904, 0.01600592, 0.69680535, 0.60190841,\n",
       "       0.5265508 , 0.12785018, 0.84017542, 0.84988268, 0.96407775,\n",
       "       0.22451064, 0.29171345, 0.59772552, 0.10365838, 0.15096797,\n",
       "       0.29171345, 0.69757121, 0.63325535, 0.94940434, 0.10570232,\n",
       "       0.55234305, 0.22075073, 0.60218239, 0.04590139, 0.22451064,\n",
       "       0.66296675, 0.68563271, 0.96392947, 0.2588914 , 0.57387334,\n",
       "       0.90148563, 0.12228942, 0.70270448, 0.57358688, 0.92525896,\n",
       "       0.12228942, 0.91490842, 0.94380606, 0.34778351, 0.21010118,\n",
       "       0.58690428, 0.18854309, 0.68760486, 0.94079319, 0.14292279,\n",
       "       0.54511099, 0.52884211, 0.19947971, 0.6623042 , 0.20933734,\n",
       "       0.14957038, 0.11451188, 0.19343962, 0.09484587, 0.60991571,\n",
       "       0.22451064, 0.27381187, 0.4580728 , 0.0191305 , 0.16493885,\n",
       "       0.47666906, 0.88040452, 0.0013739 , 0.00110529, 0.7991718 ,\n",
       "       0.24815686, 0.72408212, 0.52098233, 0.9685979 , 0.82661579,\n",
       "       0.53882248, 0.57801176, 0.18387854, 0.61666853, 0.16411794,\n",
       "       0.48760911, 0.00146971, 0.55552095, 0.27657347, 0.09384011,\n",
       "       0.35019159, 0.70898437, 0.15186137, 0.39454864, 0.02304967,\n",
       "       0.76623903, 0.20847432, 0.14168189, 0.72617911, 0.20860402,\n",
       "       0.55252253, 0.44448322, 0.49934051, 0.30599101, 0.68461945,\n",
       "       0.26159102, 0.10616813, 0.01239256])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select ONLY the probabilities referring to 1s\n",
    "predicted_proba[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Final test is to evaluate whether our model is successful or not\n",
    "\n",
    "+ Receiver Operating Characteristc(ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#AUC score for the case is 0.82. AUC score 1 represents perfect classifier, and 0.5 represents a worthless classifier.\n",
    "y_pred_proba = reg.predict_proba(x_train)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_train,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_train, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model we implemented in steps so far to build a module.\n",
    "\n",
    "By saving the model we create a module, for when we have new data with same attributes we just need to feed it to module and it will output the predictions and probability\n",
    "\n",
    "+ We are going to save our model using pickle\n",
    "\n",
    "#### What is pickle?\n",
    "\n",
    "+ There are several popular ways to save a model. To name some, you can use Joblib (a part of the SciPy ecosystem), and JSON. Certainly, each of those choices has its pros and cons. Pickle is probably the most intuitive choice of companys\n",
    "\n",
    "+ Pickle is the standard Python tool for serialization and deserialization. In simple words, pickling means: converting a Python object (no matter what) into a string of characters. Logically, unpickling is about converting a string of characters (that has been pickled) into a Python object.\n",
    "\n",
    "\n",
    "\n",
    "There are some potential issues with using pickle:\n",
    "\n",
    "+ Pickle and Python version.\n",
    "\n",
    "+ Pickling is strictly related to Python version. It is not recommended to (de)serialize objects across different Python versions. Logically, if you’re working on your own this will never be an issue (unless you upgrade/downgrade your Python version). \n",
    "\n",
    "\n",
    "\n",
    "+ Pickle is slow.\n",
    "But, you will barely notice that. However for complex structures it may take loads of time to pickle and unpickle.\n",
    "\n",
    "\n",
    "\n",
    "+ Pickle is not secure.\n",
    "This is evident from the documentation of pickle, quote: “Never unpickle data received from an untrusted or unauthenticated source.” The reason is that just about \"anything can be pickled\", so you can easily unpickle malicious code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant module\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the model file\n",
    "with open('model', 'wb') as file:\n",
    "    pickle.dump(reg, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is model and wb?\n",
    "\n",
    "+ 'model' is simply the file name in which we are saving our model. 'wb' is write bytes.\n",
    "\n",
    "+ When we pickle we use write bytes \"wb\" and when we unpickle we use read bytes \"rb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the scaler file\n",
    "with open('scaler','wb') as file:\n",
    "    pickle.dump(absenteeism_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Also we need to save Absenteeism_scaler column we created which was used to standardize all numerical variables.\n",
    "\n",
    "+ Before creating 'scaler' our code was dependent on training data. But once model is trained and we have obtained the coefficients, we can save it. Which means are actually seperating the model from training data.\n",
    "\n",
    "+ The information in \"Absenteeism_scaler\" which we will store in \"scaler\" will be used to preprocess any new data the way we did for training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
